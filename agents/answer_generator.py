# agents/answer_generator.py

from typing import Dict
from langchain_ollama import ChatOllama
from dotenv import load_dotenv
import os
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
load_dotenv()

model = ChatOllama(
    model=os.getenv("OLLAMA_MODEL", "exaone3.5:7.8b"),
    temperature=0.5,
)

def answer_generator(state: Dict) -> Dict:
    user_query = state["messages"][-1]
    rag_info = state.get("rag_result", "")
    web_info = state.get("search_result", "")
    crud_info = state.get("crud_result", "")
    prev_answer = state.get("final_answer", "")  # answer_planner/ì´ì „ ì—ì´ì „íŠ¸ ë‹µë³€

    prompt = f"""
    ë‹¹ì‹ ì€ 'ìš”ì‹ì—… ìì˜ì—…ì'ë¥¼ ë„ì™€ì£¼ëŠ” ì‹¤ë¬´ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.  
ì„¸ë¬´, ìœ„ìƒ, ì¼ì •, ë¯¼ì› ëŒ€ì‘ ë“± ì‹¤ìƒí™œì—ì„œ ë§ˆì£¼ì¹˜ëŠ” í–‰ì •Â·ì •ë³´ì  ì´ìŠˆë¥¼ **ì „ë¬¸ì ì´ë˜ ì¹œì ˆí•œ ìƒë‹´ í†¤**ìœ¼ë¡œ ë„ì™€ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
\"\"\"{user_query}\"\"\"  

[ë¬¸ì„œ ê¸°ë°˜ ì •ë³´ (RAG)]
\"\"\"{rag_info if rag_info else "ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì •ë³´ ì—†ìŒ"}\"\"\"  

[ì›¹ ê²€ìƒ‰ ê²°ê³¼]
\"\"\"{web_info if web_info else "ê´€ë ¨ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"}\"\"\"  

[ì¼ì • ì •ë³´ ë˜ëŠ” ì²˜ë¦¬ ê²°ê³¼]
\"\"\"{crud_info if crud_info else "ì¼ì •/ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ"}\"\"\"  

[ì´ì „ ìƒì„±ëœ ì‘ë‹µ ë˜ëŠ” ì´ˆì•ˆ (AnswerPlanner)]
\"\"\"{prev_answer if prev_answer else "ì´ì „ ë‹µë³€ ì—†ìŒ"}\"\"\"  

---

ì‘ì„± ì§€ì¹¨:
- **ì¼ì • ì •ë³´ ë˜ëŠ” ì²˜ë¦¬ ë°©ë²•**ì´ ìˆë‹¤ë©´ ê°€ì¥ ë¨¼ì € ì•ˆë‚´í•˜ì„¸ìš”.
- ëª¨ë“  ì •ë³´(RAG/ì›¹/ì´ì „ ë‹µë³€ ë“±)ë¥¼ ì¢…í•©í•˜ì—¬, ì¤‘ë³µ ì—†ì´ í•µì‹¬ë§Œ ì •ë¦¬í•´ ì‹¤ë¬´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- í•„ìš” ì‹œ ê´€ë ¨ ë°°ê²½ ì§€ì‹ë„ ê°„ë‹¨íˆ ë§ë¶™ì´ë˜, ë³µì¡í•œ ë²•ë ¹ í•´ì„ë³´ë‹¤ëŠ” **ì‹¤í–‰ ì¤‘ì‹¬**ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- ë‹µë³€ í†¤ì€ â€œë™ë„¤ ì„¸ë¬´ì‚¬/ìƒë‹´ì‚¬/ë²•ë¬´ì‚¬ì²˜ëŸ¼ ë„ë©”ì¸ ì§€ì‹ì— ëŒ€í•œ ì „ë¬¸ì„±ì„ ë°”íƒ•ìœ¼ë¡œ, ìì˜ì—…ìë“¤ì´ ì´í•´í•  ìˆ˜ ìˆê²Œ ì¹œì ˆí•˜ê²Œâ€ ìœ ì§€í•´ì£¼ì„¸ìš”.
- ë§ˆì§€ë§‰ì—” **ì‚¬ìš©ìì˜ í›„ì† ì§ˆë¬¸ì„ ìœ ë„**í•˜ê±°ë‚˜ **ê´€ë ¨ ì´ìŠˆì— ëŒ€í•œ ì•ˆë‚´**ë¡œ ë§ˆë¬´ë¦¬í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.

ğŸ‘‰ ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ ë˜ëŠ” ìš”ì•½í˜• ì‘ë‹µì„ ì•„ë˜ì— ì‘ì„±í•´ì£¼ì„¸ìš”. (1íšŒ ì¶œë ¥ë§Œ)

    """

    response = model.invoke(prompt)
    final_response = response.content.strip()

    state["final_output"] = final_response
    state.setdefault("agent_messages", []).append({
        "agent": "answer_generator",
        "input_snapshot": {
            "user_query": user_query,
            "rag_info": rag_info,
            "web_info": web_info,
            "crud_info": crud_info,
            "prev_answer": prev_answer
        },
        "output": final_response
    })

    return state
